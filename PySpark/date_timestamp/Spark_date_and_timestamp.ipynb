{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a852c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[3]\").appName(\"dateApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c890f7",
   "metadata": {},
   "source": [
    "## to_date(col,format=None)\n",
    "+ Converts string to date format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "04ec3637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t: string (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.printSchema()\n",
    "df.select(to_date(\"t\").alias('date')).show()\n",
    "df.select(to_date(\"t\").alias('date')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd9306",
   "metadata": {},
   "source": [
    "+ This is same as calling cast on string column with default format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "de7242df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         t|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"t\").cast(\"date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a945d4",
   "metadata": {},
   "source": [
    "+ Returns null if the format of the string in the column is not default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a8701e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|date|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame([('1997/02/28 10:30:00',)], ['t1'])\n",
    "df_1.select(to_date(\"t1\").alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a61dd",
   "metadata": {},
   "source": [
    "+ Format should match the input column values . 'yyyy/MM/dd hh:mm:ss' in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9c1ce44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.select(to_date(\"t1\",\"yyy/MM/dd hh:mm:ss\").alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b363f1",
   "metadata": {},
   "source": [
    "## date_format(date,format)\n",
    "+ Converts a date/timestamp/string to a value of string \n",
    "+ in the format specified by the date format given by the second argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57582437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|04/08/2015|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(date_format('dt', 'MM/dd/yyy').alias('date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2145e",
   "metadata": {},
   "source": [
    "## date_add(start,days) and date_sub(start,days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7c22856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| next_date|\n",
      "+----------+\n",
      "|2021-04-09|\n",
      "+----------+\n",
      "\n",
      "+--------------+\n",
      "|one_year_later|\n",
      "+--------------+\n",
      "|    2022-04-08|\n",
      "+--------------+\n",
      "\n",
      "+----------+\n",
      "| prev_date|\n",
      "+----------+\n",
      "|2021-04-07|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub\n",
    "\n",
    "df = spark.createDataFrame([('2021-04-08',)], ['dt'])\n",
    "df.select(date_add(df.dt, 1).alias('next_date')).show()\n",
    "df.select(date_add(df.dt, 365).alias('one_year_later')).show()\n",
    "\n",
    "df.select(date_sub(df.dt, 1).alias('prev_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6ab66",
   "metadata": {},
   "source": [
    "## datediff(end,start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cea85974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|datediff(dt2, dt1)|\n",
      "+------------------+\n",
      "|               132|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,datediff\n",
    "\n",
    "df = spark.createDataFrame([('2021-03-18','2021-07-28')], ['dt1','dt2'])\n",
    "df.select(datediff(col(\"dt2\"),col(\"dt1\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4de025f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|datediff(dt2, dt1)|\n",
      "+------------------+\n",
      "|               132|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(datediff(\"dt2\",\"dt1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e3f10",
   "metadata": {},
   "source": [
    "## monthsbetween(date1,date2,roundOff=True)\n",
    "\n",
    "+ Returns number of months between dates date1 and date2.\n",
    "+ If date1 is later than date2, then the result is positive. \n",
    "+ If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). \n",
    "+ The result is rounded off to 8 digits unless roundOff is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c72bc665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------+\n",
      "|       dt2|       dt1|months_between(dt2, dt1, true)|\n",
      "+----------+----------+------------------------------+\n",
      "|2021-07-28|2021-03-18|                    4.32258065|\n",
      "+----------+----------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "df = spark.createDataFrame([('2021-03-18','2021-07-28')], ['dt1','dt2'])\n",
    "df.select(\"dt2\",\"dt1\",months_between(\"dt2\",\"dt1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d8233606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------------------+\n",
      "|       dt2|       dt1|months_between(dt2, dt1, false)|\n",
      "+----------+----------+-------------------------------+\n",
      "|2021-07-28|2021-03-18|               4.32258064516129|\n",
      "+----------+----------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dt2\",\"dt1\",months_between(\"dt2\",\"dt1\",False)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f70923",
   "metadata": {},
   "source": [
    "## dayofmonth(col), dayofweek(col) ,dayofyear(col), weekofyear(col)\n",
    "\n",
    "+ dayofweek starts from Sunday by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8b319c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek,dayofmonth,dayofyear,weekofyear\n",
    "\n",
    "df = spark.createDataFrame([('2021-03-18',)], ['dt1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7a751b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------------+--------------+---------------+\n",
      "|       dt1|dayofweek(dt1)|dayofmonth(dt1)|dayofyear(dt1)|weekofyear(dt1)|\n",
      "+----------+--------------+---------------+--------------+---------------+\n",
      "|2021-03-18|             5|             18|            77|             11|\n",
      "+----------+--------------+---------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dt1\",dayofweek(\"dt1\"),dayofmonth(\"dt1\"),dayofyear(\"dt1\"),weekofyear(\"dt1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3121536",
   "metadata": {},
   "source": [
    "## last_day(date) \n",
    "\n",
    "+ Returns last_day of the month of the date passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "796284dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fe6217cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|       dt1|last_day(dt1)|\n",
      "+----------+-------------+\n",
      "|2021-03-18|   2021-03-31|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dt1\",last_day(\"dt1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c2653",
   "metadata": {},
   "source": [
    "## next_day(date,dayofweek)\n",
    "\n",
    "+ Returns the first date which is later than the value of the date column.\n",
    "+ Day of the week parameter is case insensitive, and accepts:\n",
    "  “Mon”, “Tue”, “Wed”, “Thu”, “Fri”, “Sat”, “Sun”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b5145554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------------+\n",
      "|       dt1|dayofweek(dt1)|next_day(dt1, Thu)|\n",
      "+----------+--------------+------------------+\n",
      "|2021-03-18|             5|        2021-03-25|\n",
      "+----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek,next_day\n",
    "\n",
    "df.select(\"dt1\",dayofweek(\"dt1\"),next_day(\"dt1\",\"Thu\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa43e66",
   "metadata": {},
   "source": [
    "# Timetsamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94467358",
   "metadata": {},
   "source": [
    "## from_unixtime(timestamp,format='uuuu-MM-dd HH:mm:ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9deb6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime,unix_timestamp\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "be794e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| unix_time|\n",
      "+----------+\n",
      "|1428476400|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
    "time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "20662594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 ts|\n",
      "+-------------------+\n",
      "|2015-04-08 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df_new = time_df.select(from_unixtime('unix_time').alias('ts'))\n",
    "time_df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c766ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       ts1|\n",
      "+----------+\n",
      "|2015/04/08|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conver from unix timestamp to specified format\n",
    "time_df_new2 = time_df.select(from_unixtime('unix_time',\"yyyy/MM/dd\").alias('ts1'))\n",
    "time_df_new2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa08786",
   "metadata": {},
   "source": [
    "## unixtimestamp(timestamp=None,format='uuuu-MM-dd HH:mm:ss')\n",
    "+ Convert time string with given pattern (‘uuuu-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), \n",
    "+ using the default timezone and the default locale, \n",
    "+ return null if fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4d7dd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|unix_timestamp(ts, yyyy-MM-dd HH:mm:ss)|\n",
      "+---------------------------------------+\n",
      "|                             1428476400|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_df_new.select(unix_timestamp(\"ts\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2657e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.unset(\"spark.sql.session.timeZone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb82d09",
   "metadata": {},
   "source": [
    "## to_timestamp(col,format=None)\n",
    "+ Default format is 'yyyy-mm-dd HH:mm:ss'\n",
    "+ Pass format value if the source value is in any other format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "38df63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     timestamp_col1|     timestamp_col2|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 10:30:00|1997/02/28 08:30:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('1997-02-28 10:30:25','1997/02/28 08:30:00')], ['timestamp_col1','timestamp_col2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09ab7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a1ce1ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_col1: string (nullable = true)\n",
      " |-- timestamp_col2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86fc9126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     timestamp_col1|     timestamp_col2|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 10:30:00|1997/02/28 08:30:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If formatting is not passed, default format of 'yyyy-mm-dd HH:mm:ss' is used\n",
    "df_t1 = df.select(to_timestamp(\"timestamp_col1\").alias(\"timestamp_col1\"),\"timestamp_col2\")\n",
    "df_t1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d6d9c019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_col1: timestamp (nullable = true)\n",
      " |-- timestamp_col2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice the change in datatype of the column passed to to_timestamp\n",
    "df_t1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b87176c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------+\n",
      "|     timestamp_col1|to_timestamp(`timestamp_col2`)|\n",
      "+-------------------+------------------------------+\n",
      "|1997-02-28 10:30:00|                          null|\n",
      "+-------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Returns null if column value is not in the default format of 'yyyy-mm-dd HH:mm:ss'\n",
    "# and expected source formatting is not passed and \n",
    "df_t2 = df_t1.select(\"timestamp_col1\",to_timestamp(\"timestamp_col2\"))\n",
    "df_t2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9481895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_col1: timestamp (nullable = true)\n",
      " |-- timestamp_col2: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+--------------+\n",
      "|     timestamp_col1|timestamp_col2|\n",
      "+-------------------+--------------+\n",
      "|1997-02-28 10:30:00|          null|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default operation is same as cast('timestamp').\n",
    "df.select(col(\"timestamp_col1\").cast(\"timestamp\"),col(\"timestamp_col2\").cast(\"timestamp\")).printSchema()\n",
    "df.select(col(\"timestamp_col1\").cast(\"timestamp\"),col(\"timestamp_col2\").cast(\"timestamp\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e04efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     timestamp_col1|     timestamp_col2|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 10:30:00|1997/02/28 08:30:00|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- timestamp_col1: timestamp (nullable = true)\n",
      " |-- timestamp_col2: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|     timestamp_col1|     timestamp_col2|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 10:30:00|1997-02-28 08:30:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass source formatting as in :to_timestamp(\"col\",'yyyy/MM/dd HH:mm:ss')\n",
    "df_t1.show()\n",
    "df_t3 = df_t1.select(\"timestamp_col1\",to_timestamp(\"timestamp_col2\",'yyyy/MM/dd HH:mm:ss').alias(\"timestamp_col2\"))\n",
    "df_t3.printSchema()\n",
    "df_t3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6b3a8",
   "metadata": {},
   "source": [
    "## hour(col), minute(col), second(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6a6b0c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     timestamp_col1|     timestamp_col2|\n",
      "+-------------------+-------------------+\n",
      "|1997-02-28 10:30:00|1997-02-28 08:30:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "133f48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------------+----------------------+---------------------+\n",
      "|     timestamp_col1|hour(timestamp_col1)|minute(timestamp_col1)|second(timestamp_col1)|month(timestamp_col1)|\n",
      "+-------------------+--------------------+----------------------+----------------------+---------------------+\n",
      "|1997-02-28 10:30:00|                  10|                    30|                     0|                    2|\n",
      "+-------------------+--------------------+----------------------+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour,minute,second,month\n",
    "\n",
    "df_t3.select(\"timestamp_col1\",hour(\"timestamp_col1\"),minute(\"timestamp_col1\"),second(\"timestamp_col1\"),month(\"timestamp_col1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4c2a0",
   "metadata": {},
   "source": [
    "## date_trunc(format,timestamp)\n",
    "+ Returns timestamp truncated to the unit specified by the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3edfdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc\n",
    "\n",
    "df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['timestamp_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "82f417c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|date_trunc(year, timestamp_col)|\n",
      "+-------------------------------+\n",
      "|            1997-01-01 00:00:00|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_trunc('year',\"timestamp_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7e4e49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_trunc(mon, timestamp_col)|\n",
      "+------------------------------+\n",
      "|           1997-02-01 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_trunc('mon',\"timestamp_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56d0455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_trunc(day, timestamp_col)|\n",
      "+------------------------------+\n",
      "|           1997-02-28 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_trunc('day',\"timestamp_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
